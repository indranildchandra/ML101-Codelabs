{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GSCV_RSCV_with_keras_sklearn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indranildchandra/ML101-Codelabs/blob/master/src/GSCV_RSCV_with_keras_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HZRd5me2Go5",
        "colab_type": "text"
      },
      "source": [
        "## Reference - [ML from Scratch Blog](https://mlfromscratch.com/gridsearch-keras-sklearn/#/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaIBhyaG2zZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.datasets import load_boston, load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO5pj5oR8VTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
        "                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n",
        "                       do_probabilities = False):\n",
        "    gs = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=param_grid, \n",
        "        cv=cv, \n",
        "        n_jobs=-1, \n",
        "        scoring=scoring_fit,\n",
        "        verbose=2\n",
        "    )\n",
        "    fitted_model = gs.fit(X_train_data, y_train_data)\n",
        "    \n",
        "    if do_probabilities:\n",
        "      pred = fitted_model.predict_proba(X_test_data)\n",
        "    else:\n",
        "      pred = fitted_model.predict(X_test_data)\n",
        "    \n",
        "    return fitted_model, pred"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxM4tWsJ8h4s",
        "colab_type": "text"
      },
      "source": [
        "## Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvGefrHe_YQ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "98366ee7-f33a-41f9-cfb6-012d030e3aba"
      },
      "source": [
        "# LOAD DATA\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# PREPROCESSING\n",
        "def preprocess_mnist(x_train, y_train, x_test, y_test):\n",
        "    # Normalizing all images of 28x28 pixels\n",
        "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "    input_shape = (28, 28, 1)\n",
        "    \n",
        "    # Float values for division\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    \n",
        "    # Normalizing the RGB codes by dividing it to the max RGB value\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "    \n",
        "    # Categorical y values\n",
        "    y_train = to_categorical(y_train, 10)\n",
        "    y_test= to_categorical(y_test, 10)\n",
        "    \n",
        "    return x_train, y_train, x_test, y_test, input_shape\n",
        "    \n",
        "X_train, y_train, X_test, y_test, input_shape = preprocess_mnist(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tog3I6d38ku0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Readying neural network model\n",
        "def build_cnn(activation = 'relu',\n",
        "              dropout_rate = 0.2,\n",
        "              optimizer = 'Adam'):\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "              activation=activation,\n",
        "              input_shape=input_shape))\n",
        "    model.add(Conv2D(64, (3, 3), activation=activation))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation=activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy', \n",
        "        optimizer=optimizer, \n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohOII7ellQbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KerasClassifier(build_fn = build_cnn, verbose=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQLFD9OH_xiq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "3e222523-9464-4e4f-9610-079807b4d156"
      },
      "source": [
        "param_grid = {\n",
        "              'epochs':[1,2,3],\n",
        "              'batch_size':[128]\n",
        "              #'epochs' :              [100,150,200],\n",
        "              #'batch_size' :          [32, 128],\n",
        "              #'optimizer' :           ['Adam', 'Nadam'],\n",
        "              #'dropout_rate' :        [0.2, 0.3],\n",
        "              #'activation' :          ['relu', 'elu']\n",
        "             }\n",
        "\n",
        "model = KerasClassifier(build_fn = build_cnn, verbose=0)\n",
        "\n",
        "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
        "                                        param_grid, cv=5, scoring_fit='neg_log_loss')\n",
        "\n",
        "print(model.best_score_)\n",
        "print(model.best_params_)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  1.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/wrappers/scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "-0.042024117985387535\n",
            "{'batch_size': 128, 'epochs': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5a2rfgo1may",
        "colab_type": "text"
      },
      "source": [
        "## Solving the problem with scoring method\n",
        "We need to remove the categorical encoding of the output datasets (y_train and y_test), for GridSearchCV to work. It has something to do with how scikit-learn converts such variables, which is different from how Keras does it. [Link for more info.](https://github.com/keras-team/keras/issues/9331)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWixrDnF1lDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOAD DATA\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# PREPROCESSING\n",
        "def preprocess_mnist(x_train, y_train, x_test, y_test):\n",
        "    # Normalizing all images of 28x28 pixels\n",
        "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "    input_shape = (28, 28, 1)\n",
        "    \n",
        "    # Float values for division\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    \n",
        "    # Normalizing the RGB codes by dividing it to the max RGB value\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "    \n",
        "    return x_train, y_train, x_test, y_test, input_shape\n",
        "    \n",
        "X_train, y_train, X_test, y_test, input_shape = preprocess_mnist(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQp9t9Vb19T3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "bc037f6f-a458-4fcf-d50b-298f09345816"
      },
      "source": [
        "param_grid = {\n",
        "              'epochs':[1,2,3],\n",
        "              'batch_size':[128]\n",
        "              #'epochs' :              [100,150,200],\n",
        "              #'batch_size' :          [32, 128],\n",
        "              #'optimizer' :           ['Adam', 'Nadam'],\n",
        "              #'dropout_rate' :        [0.2, 0.3],\n",
        "              #'activation' :          ['relu', 'elu']\n",
        "             }\n",
        "\n",
        "model = KerasClassifier(build_fn = build_cnn, verbose=0)\n",
        "\n",
        "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
        "                                        param_grid, cv=5, scoring_fit='accuracy')\n",
        "\n",
        "print(model.best_score_)\n",
        "print(model.best_params_)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  1.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9868833333333333\n",
            "{'batch_size': 128, 'epochs': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOneDewQ8f8u",
        "colab_type": "text"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3clcGa6P_LPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import boston house prices dataset\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXhndZox8ebr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3fecf106-3760-4127-a4c8-c410f7998264"
      },
      "source": [
        "model = xgb.XGBRegressor()\n",
        "param_grid = {\n",
        "    'n_estimators': [400, 700, 1000],\n",
        "    'colsample_bytree': [0.7, 0.8],\n",
        "    'max_depth': [15,20,25],\n",
        "    'reg_alpha': [1.1, 1.2, 1.3],\n",
        "    'reg_lambda': [1.1, 1.2, 1.3],\n",
        "    'subsample': [0.7, 0.8, 0.9]\n",
        "}\n",
        "\n",
        "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
        "                                 param_grid, cv=5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   10.0s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:   42.9s\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1009 tasks      | elapsed:  5.8min\n",
            "[Parallel(n_jobs=-1)]: Done 1454 tasks      | elapsed:  8.4min\n",
            "[Parallel(n_jobs=-1)]: Done 1981 tasks      | elapsed: 11.9min\n",
            "[Parallel(n_jobs=-1)]: Done 2430 out of 2430 | elapsed: 14.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[17:56:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs9V2w1s1eOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "40ccc279-03e5-4e24-a819-980740dfb6fa"
      },
      "source": [
        "# Root Mean Squared Error\n",
        "print(np.sqrt(-model.best_score_))\n",
        "print(model.best_params_)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.4853992531840245\n",
            "{'colsample_bytree': 0.8, 'max_depth': 20, 'n_estimators': 400, 'reg_alpha': 1.2, 'reg_lambda': 1.3, 'subsample': 0.8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ZKkzgi8Rjj",
        "colab_type": "text"
      },
      "source": [
        "# LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsSpnJ8JE8bZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UabzOyPU67F5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "a3708ff7-4bb1-49bd-d6ef-b9abdc606fb2"
      },
      "source": [
        "model = lgb.LGBMClassifier()\n",
        "param_grid = {\n",
        "    'n_estimators': [400, 700, 1000],\n",
        "    'colsample_bytree': [0.7, 0.8],\n",
        "    'max_depth': [15,20,25],\n",
        "    'num_leaves': [50, 100, 200],\n",
        "    'reg_alpha': [1.1, 1.2, 1.3],\n",
        "    'reg_lambda': [1.1, 1.2, 1.3],\n",
        "    'min_split_gain': [0.3, 0.4],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'subsample_freq': [20]\n",
        "}\n",
        "\n",
        "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
        "                                 param_grid, cv=5, scoring_fit='accuracy')\n",
        "\n",
        "print(model.best_score_)\n",
        "print(model.best_params_)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 2916 candidates, totalling 14580 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:    2.5s\n",
            "[Parallel(n_jobs=-1)]: Done 312 tasks      | elapsed:   10.6s\n",
            "[Parallel(n_jobs=-1)]: Done 718 tasks      | elapsed:   29.4s\n",
            "[Parallel(n_jobs=-1)]: Done 1284 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=-1)]: Done 2014 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done 2904 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=-1)]: Done 3958 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=-1)]: Done 5172 tasks      | elapsed:  4.2min\n",
            "[Parallel(n_jobs=-1)]: Done 6550 tasks      | elapsed:  5.4min\n",
            "[Parallel(n_jobs=-1)]: Done 8088 tasks      | elapsed:  6.7min\n",
            "[Parallel(n_jobs=-1)]: Done 9790 tasks      | elapsed:  8.3min\n",
            "[Parallel(n_jobs=-1)]: Done 11652 tasks      | elapsed:  9.9min\n",
            "[Parallel(n_jobs=-1)]: Done 13678 tasks      | elapsed: 11.8min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9736263736263737\n",
            "{'colsample_bytree': 0.7, 'max_depth': 15, 'min_split_gain': 0.3, 'n_estimators': 400, 'num_leaves': 50, 'reg_alpha': 1.1, 'reg_lambda': 1.2, 'subsample': 0.7, 'subsample_freq': 20}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 14580 out of 14580 | elapsed: 12.7min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IduhYUQ38wTN",
        "colab_type": "text"
      },
      "source": [
        "# Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq9vDPPE2ycD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoQ0jVEc8vnr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8583b13a-c5e0-4519-a648-cfbf195a8f5a"
      },
      "source": [
        "model = RandomForestClassifier()\n",
        "param_grid = {\n",
        "    'n_estimators': [400, 700, 1000],\n",
        "    'max_depth': [15,20,25],\n",
        "    'max_leaf_nodes': [50, 100, 200]\n",
        "}\n",
        "\n",
        "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
        "                                 param_grid, cv=5, scoring_fit='accuracy')\n",
        "\n",
        "print(model.best_score_)\n",
        "print(model.best_params_)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   33.3s\n",
            "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9626373626373625\n",
            "{'max_depth': 20, 'max_leaf_nodes': 100, 'n_estimators': 400}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DsS_q-bqihq",
        "colab_type": "text"
      },
      "source": [
        "# RandomSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F704qh7cqf-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def search_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
        "                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n",
        "                       do_probabilities = False, search_mode = 'GridSearchCV', n_iterations = 0):\n",
        "    fitted_model = None\n",
        "    \n",
        "    if(search_mode == 'GridSearchCV'):\n",
        "        gs = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grid, \n",
        "            cv=cv, \n",
        "            n_jobs=-1, \n",
        "            scoring=scoring_fit,\n",
        "            verbose=2\n",
        "        )\n",
        "        fitted_model = gs.fit(X_train_data, y_train_data)\n",
        "\n",
        "    elif (search_mode == 'RandomizedSearchCV'):\n",
        "        rs = RandomizedSearchCV(\n",
        "            estimator=model,\n",
        "            param_distributions=param_grid, \n",
        "            cv=cv,\n",
        "            n_iter=n_iterations,\n",
        "            n_jobs=-1, \n",
        "            scoring=scoring_fit,\n",
        "            verbose=2\n",
        "        )\n",
        "        fitted_model = rs.fit(X_train_data, y_train_data)\n",
        "    \n",
        "    \n",
        "    if(fitted_model != None):\n",
        "        if do_probabilities:\n",
        "            pred = fitted_model.predict_proba(X_test_data)\n",
        "        else:\n",
        "            pred = fitted_model.predict(X_test_data)\n",
        "            \n",
        "        return fitted_model, pred\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dso7yIy6wPhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuKwVKapwSgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8a600aac-ee69-46ea-9dd8-6a688b34ba94"
      },
      "source": [
        "model = RandomForestClassifier()\n",
        "param_grid = {\n",
        "    'n_estimators': [400, 700, 1000],\n",
        "    'max_depth': [15,20,25],\n",
        "    'max_leaf_nodes': [50, 100, 200]\n",
        "}\n",
        "\n",
        "model, pred = search_pipeline(X_train, X_test, y_train, y_test, model, \n",
        "                                 param_grid, cv=5, scoring_fit='accuracy',\n",
        "                                 search_mode = 'RandomizedSearchCV', n_iterations = 15)\n",
        "\n",
        "print(model.best_score_)\n",
        "print(model.best_params_)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   42.0s\n",
            "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:  1.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9648351648351647\n",
            "{'n_estimators': 700, 'max_leaf_nodes': 50, 'max_depth': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}