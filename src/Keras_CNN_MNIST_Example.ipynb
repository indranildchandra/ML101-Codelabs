{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_CNN_MNIST_Example",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indranildchandra/ML101-Codelabs/blob/master/src/Keras_CNN_MNIST_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8Pu332d1_rq",
        "colab_type": "text"
      },
      "source": [
        "# **Step 1: Import required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkqT0C2t12uO",
        "colab_type": "code",
        "outputId": "667cce25-f344-425a-aeb4-3bfee744c5f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Lambda, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, BatchNormalization\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQCXjd4M3S7p",
        "colab_type": "text"
      },
      "source": [
        "# **Step 2: Define all helper functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cRIm91e5FmP",
        "colab_type": "text"
      },
      "source": [
        "**normalize** \n",
        "\n",
        "-> this method normalizes the entire dataset \n",
        "\n",
        "-> i.e. subtract mean value of the distribution from each data point and divide it by standard deviation of the distribution\n",
        "\n",
        "-> this helps in transforming the entire dataset into values just between 0 and 1, helps avoiding memory overflow due to large numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wbeyIJH5LEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(x):\n",
        "    x_mean = x_train.mean().astype(np.float32)\n",
        "    x_std = x_train.std().astype(np.float32)\n",
        "    return (x - x_mean) / x_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvJxK2bx7Ecw",
        "colab_type": "text"
      },
      "source": [
        "**onehot** \n",
        "\n",
        "-> this method performs “binarization” of the category and include it as a feature to train the model\n",
        "\n",
        "-> Suppose you have ‘flower’ feature which can take values ‘daffodil’, ‘lily’, and ‘rose’. One hot encoding converts ‘flower’ feature to three features, ‘is_daffodil’, ‘is_lily’, and ‘is_rose’ which all are binary. \n",
        "\n",
        "![Onwhot Encoding](https://cdn-images-1.medium.com/max/1200/1*Ac4z1rWWuU0TzxJRUM62WA.jpeg)\n",
        "*Courtesy - hackernoon.com*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1-Du5cG6V9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def onehot(y):\n",
        "    return keras.utils.np_utils.to_categorical(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2ta0S8e8hL5",
        "colab_type": "text"
      },
      "source": [
        "**get_data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLu36hXY8Mrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(sample=False):\n",
        "    if sample:\n",
        "        x,y = x_train_sample, y_train_sample\n",
        "        val_x, val_y = x_test_sample, y_test_sample\n",
        "    else:\n",
        "        x,y = x_train, y_train\n",
        "        val_x, val_y = x_test, y_test\n",
        "    return x, y, val_x, val_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Np6ator3pbX",
        "colab_type": "text"
      },
      "source": [
        "**basic_model**\n",
        "\n",
        "-> Input Shape is 28 X 28 X 1\n",
        "\n",
        "-> Normalize the input before feeding into your Neural Net\n",
        "\n",
        "-> Lambda - wraps arbitrary expression as a Layer object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWCUuIRu8ozB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def basic_model():\n",
        "    return Sequential([Lambda(normalize, input_shape=(28,28,1))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfzLP6id8q_x",
        "colab_type": "text"
      },
      "source": [
        "**add_output_layer**\n",
        "\n",
        "-> Activation = Softmax function calculates the probabilities distribution of the event over ‘n’ different events. In general way of saying, this function will calculate the probabilities of each target class over all possible target classes. Later the calculated probabilities will be helpful for determining the target class for the given inputs.\n",
        "\n",
        "The main advantage of using Softmax is the output probabilities range. The range will 0 to 1, and the sum of all the probabilities will be equal to one. If the softmax function used for multi-classification model it returns the probabilities of each class and the target class will have the high probability.\n",
        "\n",
        "The formula computes the exponential (e-power) of the given input value and the sum of exponential values of all the values in the inputs. Then the ratio of the exponential of the input value and the sum of exponential values is the output of the softmax function.\n",
        "\n",
        "Properties:\n",
        "1.  The calculated probabilities will be in the range of 0 to 1.\n",
        "2.  The sum of all the probabilities is equals to 1.\n",
        "\n",
        "\n",
        "Softmax Function Usage:\n",
        "1.  Used in multiple classification logistic regression model.\n",
        "2.  In building neural networks softmax functions used in different layer level.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzYisZ3h8wO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_output_layer(model):\n",
        "    model.add(Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwWqkMN488to",
        "colab_type": "text"
      },
      "source": [
        "**convolutional_model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDSIWfnR8-Cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convolutional_model(conv_blocks=2, hidden_dense_activation=\"relu\", pool_size=2, num_dense_neurons=512, starting_filter_size=32, batchnorm=False, zero_padding=True, dropout=False):\n",
        "    model = basic_model()\n",
        "    \n",
        "    for block in range(conv_blocks):\n",
        "        num_filters = starting_filter_size * (block + 1)\n",
        "        \n",
        "        if zero_padding:\n",
        "            model.add(ZeroPadding2D())\n",
        "        model.add(Convolution2D(num_filters, 3, 3, activation=\"relu\"))\n",
        "        \n",
        "        if batchnorm:\n",
        "            model.add(BatchNormalization(axis=1))\n",
        "        model.add(Convolution2D(num_filters, 3, 3, activation=\"relu\"))\n",
        "        \n",
        "        model.add(MaxPooling2D(pool_size=pool_size))\n",
        "        \n",
        "        if batchnorm and block is not conv_blocks - 1:\n",
        "            model.add(BatchNormalization(axis=1))\n",
        "            \n",
        "    model.add(Flatten())\n",
        "    \n",
        "    if batchnorm:\n",
        "        model.add(BatchNormalization(axis=1))\n",
        "    model.add(Dense(num_dense_neurons, activation=hidden_dense_activation))\n",
        "    \n",
        "    if batchnorm:\n",
        "        model.add(BatchNormalization(axis=1))\n",
        "    \n",
        "    if dropout:\n",
        "        model.add(Dropout(dropout))\n",
        "    \n",
        "    add_output_layer(model)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaXCiAs-9F24",
        "colab_type": "text"
      },
      "source": [
        "**compile_model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQpBVk-69LnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compile_model(model):\n",
        "    # Categorical cross entropy is used when you have more than 2 classes to compare against.\n",
        "    # We have 10 classes for MNIST (digits 0 - 9), so thus we use it here.\n",
        "    return model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv5c3ieC9M1c",
        "colab_type": "text"
      },
      "source": [
        "**fit**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTtpcC1v9Qi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(model, learning_rate=None, epochs=1, sample=False):\n",
        "    \n",
        "    model.optimizer.lr = learning_rate if learning_rate else model.optimizer.lr\n",
        "    \n",
        "    x, y, val_x, val_y = get_data(sample=sample)\n",
        "    \n",
        "    model.fit(x, y, batch_size=64, epochs=epochs, validation_data=(val_x, val_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LkC49ZK9XfJ",
        "colab_type": "text"
      },
      "source": [
        "**multi_fit**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Grhu2vq9WT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_fit(model, reset=True, augmentation=False, sample=False, epochs=1, runs=1):\n",
        "    for run in range(runs):\n",
        "        for learning_rate in [0.001, 0.01, 0.1]:\n",
        "            print(\"Fitting with learning rate of: \", learning_rate)\n",
        "            \n",
        "            fit(model, learning_rate=learning_rate, epochs=epochs, sample=sample)\n",
        "            \n",
        "            if reset:\n",
        "                # Resetting is nice here for comparing differences in learning rate, without the compounding factor of model state across epochs\n",
        "                model.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kUS5nN0-uuC",
        "colab_type": "text"
      },
      "source": [
        "# **Step 3: Define all executor functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEOEzEFm9bXK",
        "colab_type": "text"
      },
      "source": [
        "**Load Data** - load MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iumuapAN9y_u",
        "colab_type": "code",
        "outputId": "90fd88dd-db34-4a38-e677-8ed49f780adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "y_train = onehot(y_train)\n",
        "y_test = onehot(y_test)\n",
        "\n",
        "sample_size = 4000\n",
        "x_train_sample = x_train[:sample_size]\n",
        "y_train_sample = y_train[:sample_size]\n",
        "x_test_sample = x_test[:sample_size]\n",
        "y_test_sample = y_test[:sample_size]\n",
        "\n",
        "\n",
        "print(\"Shape of Input Data : \", x_train_sample.shape)\n",
        "print(\"Shape of Output Labels : \",y_train_sample.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Shape of Input Data :  (4000, 28, 28, 1)\n",
            "Shape of Output Labels :  (4000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQZ8Jw8h-Lqp",
        "colab_type": "text"
      },
      "source": [
        "**Define Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7dCC-Xg-b8j",
        "colab_type": "code",
        "outputId": "d987d59b-6929-429b-90b2-3dadb9a58438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "model = convolutional_model(conv_blocks=1, starting_filter_size=3, pool_size=(4,4), hidden_dense_activation=\"relu\", num_dense_neurons=8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(3, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(3, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdGtkxJV-et6",
        "colab_type": "text"
      },
      "source": [
        "**Compile Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiLOVNUf-g_Y",
        "colab_type": "code",
        "outputId": "bd087720-9722-4d54-8199-b7767eaf885f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "compile_model(model)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_1 (Lambda)            (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPaddin (None, 30, 30, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 3)         30        \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 3)         84        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 3)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 108)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 872       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                90        \n",
            "=================================================================\n",
            "Total params: 1,076\n",
            "Trainable params: 1,076\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciT-lSjD-i9a",
        "colab_type": "text"
      },
      "source": [
        "**Optimize Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjHuH-2--lgg",
        "colab_type": "code",
        "outputId": "f3d04c0a-f3f8-480a-95f6-76af9cdb9304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3077
        }
      },
      "source": [
        "multi_fit(model, sample=True, epochs=2, runs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting with learning rate of:  0.001\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 5s 1ms/step - loss: 2.1843 - acc: 0.1995 - val_loss: 2.0305 - val_acc: 0.3033\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 1.7095 - acc: 0.4552 - val_loss: 1.5855 - val_acc: 0.5005\n",
            "Fitting with learning rate of:  0.01\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 1.2437 - acc: 0.6082 - val_loss: 1.2159 - val_acc: 0.6188\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.9369 - acc: 0.6875 - val_loss: 0.9894 - val_acc: 0.6773\n",
            "Fitting with learning rate of:  0.1\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.7597 - acc: 0.7485 - val_loss: 0.8843 - val_acc: 0.7090\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6585 - acc: 0.7938 - val_loss: 0.7841 - val_acc: 0.7502\n",
            "Fitting with learning rate of:  0.001\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.5928 - acc: 0.8135 - val_loss: 0.7269 - val_acc: 0.7732\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.5392 - acc: 0.8337 - val_loss: 0.6827 - val_acc: 0.7820\n",
            "Fitting with learning rate of:  0.01\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.5005 - acc: 0.8458 - val_loss: 0.6376 - val_acc: 0.7937\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.4667 - acc: 0.8552 - val_loss: 0.5990 - val_acc: 0.8090\n",
            "Fitting with learning rate of:  0.1\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.4420 - acc: 0.8653 - val_loss: 0.5893 - val_acc: 0.8055\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4182 - acc: 0.8685 - val_loss: 0.5461 - val_acc: 0.8215\n",
            "Fitting with learning rate of:  0.001\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3963 - acc: 0.8780 - val_loss: 0.5266 - val_acc: 0.8300\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.3779 - acc: 0.8860 - val_loss: 0.5134 - val_acc: 0.8283\n",
            "Fitting with learning rate of:  0.01\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.3637 - acc: 0.8872 - val_loss: 0.4711 - val_acc: 0.8455\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.3451 - acc: 0.8940 - val_loss: 0.4506 - val_acc: 0.8518\n",
            "Fitting with learning rate of:  0.1\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.3253 - acc: 0.8997 - val_loss: 0.4319 - val_acc: 0.8562\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2912 - acc: 0.9107 - val_loss: 0.4129 - val_acc: 0.8675\n",
            "Fitting with learning rate of:  0.001\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2746 - acc: 0.9170 - val_loss: 0.3739 - val_acc: 0.8812\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2606 - acc: 0.9225 - val_loss: 0.3630 - val_acc: 0.8818\n",
            "Fitting with learning rate of:  0.01\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2514 - acc: 0.9260 - val_loss: 0.3634 - val_acc: 0.8828\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2398 - acc: 0.9270 - val_loss: 0.3447 - val_acc: 0.8895\n",
            "Fitting with learning rate of:  0.1\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2323 - acc: 0.9325 - val_loss: 0.3338 - val_acc: 0.8942\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2280 - acc: 0.9327 - val_loss: 0.3311 - val_acc: 0.8915\n",
            "Fitting with learning rate of:  0.001\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2191 - acc: 0.9350 - val_loss: 0.3276 - val_acc: 0.8940\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2156 - acc: 0.9353 - val_loss: 0.3200 - val_acc: 0.8982\n",
            "Fitting with learning rate of:  0.01\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2069 - acc: 0.9382 - val_loss: 0.3102 - val_acc: 0.8988\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2072 - acc: 0.9377 - val_loss: 0.3216 - val_acc: 0.9000\n",
            "Fitting with learning rate of:  0.1\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2029 - acc: 0.9420 - val_loss: 0.3032 - val_acc: 0.9015\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1935 - acc: 0.9467 - val_loss: 0.2996 - val_acc: 0.9028\n",
            "Fitting with learning rate of:  0.001\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1913 - acc: 0.9430 - val_loss: 0.3072 - val_acc: 0.9030\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1893 - acc: 0.9463 - val_loss: 0.2964 - val_acc: 0.9035\n",
            "Fitting with learning rate of:  0.01\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1830 - acc: 0.9475 - val_loss: 0.2896 - val_acc: 0.9060\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1820 - acc: 0.9470 - val_loss: 0.3048 - val_acc: 0.9065\n",
            "Fitting with learning rate of:  0.1\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1819 - acc: 0.9450 - val_loss: 0.2838 - val_acc: 0.9100\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1743 - acc: 0.9467 - val_loss: 0.2961 - val_acc: 0.9060\n",
            "Fitting with learning rate of:  0.001\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1732 - acc: 0.9483 - val_loss: 0.2930 - val_acc: 0.9055\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1740 - acc: 0.9510 - val_loss: 0.2988 - val_acc: 0.9058\n",
            "Fitting with learning rate of:  0.01\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1686 - acc: 0.9505 - val_loss: 0.2876 - val_acc: 0.9073\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1680 - acc: 0.9527 - val_loss: 0.2902 - val_acc: 0.9050\n",
            "Fitting with learning rate of:  0.1\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1658 - acc: 0.9515 - val_loss: 0.2963 - val_acc: 0.9080\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1597 - acc: 0.9543 - val_loss: 0.2834 - val_acc: 0.9085\n",
            "Fitting with learning rate of:  0.001\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1604 - acc: 0.9510 - val_loss: 0.2923 - val_acc: 0.9073\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1577 - acc: 0.9540 - val_loss: 0.2840 - val_acc: 0.9093\n",
            "Fitting with learning rate of:  0.01\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1558 - acc: 0.9530 - val_loss: 0.2789 - val_acc: 0.9127\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1528 - acc: 0.9567 - val_loss: 0.2799 - val_acc: 0.9113\n",
            "Fitting with learning rate of:  0.1\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1519 - acc: 0.9565 - val_loss: 0.2842 - val_acc: 0.9100\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1469 - acc: 0.9587 - val_loss: 0.2793 - val_acc: 0.9093\n",
            "Fitting with learning rate of:  0.001\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1477 - acc: 0.9580 - val_loss: 0.2792 - val_acc: 0.9093\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1459 - acc: 0.9580 - val_loss: 0.2744 - val_acc: 0.9130\n",
            "Fitting with learning rate of:  0.01\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1425 - acc: 0.9613 - val_loss: 0.2734 - val_acc: 0.9125\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1427 - acc: 0.9592 - val_loss: 0.2833 - val_acc: 0.9087\n",
            "Fitting with learning rate of:  0.1\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1422 - acc: 0.9597 - val_loss: 0.2746 - val_acc: 0.9140\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1377 - acc: 0.9590 - val_loss: 0.2839 - val_acc: 0.9087\n",
            "Fitting with learning rate of:  0.001\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1370 - acc: 0.9617 - val_loss: 0.2729 - val_acc: 0.9127\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1357 - acc: 0.9617 - val_loss: 0.2796 - val_acc: 0.9100\n",
            "Fitting with learning rate of:  0.01\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1358 - acc: 0.9613 - val_loss: 0.2775 - val_acc: 0.9145\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1375 - acc: 0.9617 - val_loss: 0.2736 - val_acc: 0.9143\n",
            "Fitting with learning rate of:  0.1\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1356 - acc: 0.9625 - val_loss: 0.2674 - val_acc: 0.9150\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.1275 - acc: 0.9645 - val_loss: 0.2751 - val_acc: 0.9125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1HAn_cxwHwp",
        "colab_type": "text"
      },
      "source": [
        "### Visualization of Model's forward propagation in excel -> https://docs.google.com/spreadsheets/d/1SwfVctd4TjdN2S8BL09ktpQN_41sARYzD3NEHyr-8Z0/ \n",
        "\n",
        "Courtesy: Jeremy Howard - Fast.ai"
      ]
    }
  ]
}